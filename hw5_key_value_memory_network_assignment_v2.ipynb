{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3M1CFw-VT5vI"
   },
   "source": [
    "#Preamble\n",
    "\n",
    "This mini-project involves working through all the steps of a problem, whereas prior assignments asked you to just implement core functions. We will give you a dataset, but you will also have the opportunity to manipulate the data in ways that you find beneficial to the overall project and to explain why and how those manipulations mattered. This will be in addition to building the model from scratch, developing the training loop, and implementing testing. The code will be accompanied by a report written into the notebook.\n",
    "\n",
    "This project will have you working with attention mechanism, in a new type of system for question-answering. This will provide you with experience working with attention mechanisms while not directly working with transformers.\n",
    "\n",
    "This assignment is not autograded. You can modify any code cells as long as you achieve the requirements of each graded component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yhZzhjGaTj9P"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "Memory networks learn to access external memory stores (a database or, in the case of this assignment, a dictionary). Key-Value Memory Networks specifically assume that the external memory store is organized as a dictionary with keys and values. In theory memory networks are useful when one wants a neural network to be able to know a lot of information but we don't want to try to encode that information directly into the parameters of the network. This means information can be changed in the external memory database without retraining the neural network.\n",
    "\n",
    "Given a question, e.g., \"Where was Alexander Hamilton born?\", a key-value memory network learns an embedding such that the question has a high cosine similarity to a particular key in the external dictionary. Because there are many keys that need to be matched against, key-value memory networks implement an attention-scoring mechanism to select a key. Because attention is a probabilistic score, the key-value memory network retrieves a sum of embeddings weighted according to the attention score. This weighted embedding is then compared to values using a second attention-scoring mechanism. The value with the highest cosine similarity can then be retrieved and returned as the answer.\n",
    "\n",
    "Memory networks were an important part of the evolution of question-answering systems that have been eclipsed by transformers. However, the attention mechanism in a key-value memory network is very similar to the self-attention inside a transformer, so implementing a key-value memory network is a really great way to experiment and learn about self-attention without the added complexity of transformers.\n",
    "\n",
    "Key-value memory networks are also closely related to retieval-based generation networks, except we will be retrieving facts from a dictionary instead of via the internet. However, the embedding of retrieved data will be similar.\n",
    "\n",
    "Key-value memory networks are described in this [paper](https://arxiv.org/abs/1606.03126). It is recommended that you read the paper, but we will also walk through the steps you will need to complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XLQP14bxNXDx"
   },
   "source": [
    "# Some imports\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FAe6-NtARNjW"
   },
   "source": [
    "You may add imports as necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "JfnqkHKkITC3"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "AviW7dfn6pUi"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQM96JBsVHwF"
   },
   "source": [
    "Unidecode is useful for getting rid of issues that arise from unicode. This should not be used if we care about unicode, but for the purposes of an instructional exercise, it eliminates a lot of edge cases that come up with unicode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "EYuVdhWfAcGQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unidecode in ./.venv/lib/python3.13/site-packages (1.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "3E_fVmV2_8ub"
   },
   "outputs": [],
   "source": [
    "import unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p_bDmt5iSD_j"
   },
   "source": [
    "If you need to have a reduced vocabulary, you can create an unknown \"unk\" token and add it to the vocabulary. Make sure the token index in the vocabulary and `UNK_ID` match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "DDHhoxzyAByu"
   },
   "outputs": [],
   "source": [
    "UNK = 'unk'\n",
    "UNK_ID = '0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sl6J8BwVpHCw"
   },
   "source": [
    "# Some utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4LVVnmlNRKTp"
   },
   "source": [
    "You may edit these as necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4KF3VeFoSenc"
   },
   "source": [
    "Stem words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "CPwG_8VrNUop"
   },
   "outputs": [],
   "source": [
    "# Stemming the text\n",
    "def simple_stemmer(text):\n",
    "    ps=nltk.porter.PorterStemmer()\n",
    "    text= [ps.stem(word) for word in text]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mDg9vUiDShG-"
   },
   "source": [
    "Simple tokenizer that only keeps letters and numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "eEofSDrQ9fg_"
   },
   "outputs": [],
   "source": [
    "def tokenize(line):\n",
    "    line = re.sub(r'[^a-zA-Z0-9]', ' ', unidecode.unidecode(line)) # remove punctuation\n",
    "    line = line.lower().split()  # lower case\n",
    "    return line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IH2QW9FjSrJh"
   },
   "source": [
    "A standard vocabulary object class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "25siAXsXOJj9"
   },
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, name = 'vocab'):\n",
    "        self.name = name\n",
    "        self._word2index = {}\n",
    "        self._word2count = {}\n",
    "        self._index2word = {}\n",
    "        self._n_words = 0\n",
    "\n",
    "    def get_words(self):\n",
    "      return list(self._word2count.keys())\n",
    "\n",
    "    def num_words(self):\n",
    "      return self._n_words\n",
    "\n",
    "    def word2index(self, word):\n",
    "      return self._word2index[word]\n",
    "\n",
    "    def index2word(self, word):\n",
    "      return self._index2word[word]\n",
    "\n",
    "    def word2count(self, word):\n",
    "      return self._word2count[word]\n",
    "\n",
    "    def add_sentence(self, sentence):\n",
    "        for word in tokenize(sentence):\n",
    "            self.add_word(word)\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self._word2index:\n",
    "            self._word2index[word] = self._n_words\n",
    "            self._word2count[word] = 1\n",
    "            self._index2word[self._n_words] = word\n",
    "            self._n_words += 1\n",
    "        else:\n",
    "            self._word2count[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "esAT3lG1S5cV"
   },
   "source": [
    "Make a bag of words frmo a sentence, given a vocabulary. Can return a bag of word counts or a a bag of word presences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "oKJP2y8g-TFE"
   },
   "outputs": [],
   "source": [
    "def multihot(s, vocab, preserve_counts = False):\n",
    "  tokens = []\n",
    "  for t in tokenize(s):\n",
    "    if t in vocab._word2index:\n",
    "      tokens.append(vocab.word2index(t))\n",
    "    else:\n",
    "      # Handle unknown words by skipping them or using UNK token if available\n",
    "      if UNK in vocab._word2index:\n",
    "        tokens.append(vocab.word2index(UNK))\n",
    "      # Otherwise skip unknown words\n",
    "  \n",
    "  if not tokens:  # If no valid tokens found, return zero vector\n",
    "    return np.zeros(vocab.num_words())\n",
    "    \n",
    "  tokens = np.array(tokens)\n",
    "  mhot = np.zeros((tokens.size, vocab.num_words()))\n",
    "  mhot[np.arange(tokens.size), tokens] = 1\n",
    "  if preserve_counts:\n",
    "    return mhot.sum(0)\n",
    "  else:\n",
    "    return mhot.sum(0) >= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4w8BflMUTLW_"
   },
   "source": [
    "If you have a reduced vocabulary, use this to replace out-of-vocab words. If you use this, you may want to merge it with `multihot` above to avoid tokenizing twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "LfvAfjEl7Z6j"
   },
   "outputs": [],
   "source": [
    "def unkit(s, vocab):\n",
    "  return ' '.join(list(map(lambda x: UNK if x not in vocab._word2index else x, tokenize(s))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YHZm-bAanrrf"
   },
   "source": [
    "# Part A: Download and Process Data (0 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u-L61V7tVPok"
   },
   "source": [
    "This dataset contains the information in tables that are commonly used in Wikipedia biography pages. Each person has different rows of information pertaining to their notable accomplishments and details about their life. There are a large number of types of information that can appear as rows in the biography tables, however they are relatively uniform. We call the keys of the rows \"relations\".\n",
    "\n",
    "For example [Alexander Hamilton](https://en.wikipedia.org/wiki/Alexander_Hamilton) has information about the President he worked for as Secretary of State, birth date, date of death, parents' names, etc.\n",
    "\n",
    "The code below will download the dataset and process it to create two things:\n",
    "- `DB`: a hash table that map titles of biography wikipedia articles to table information. The table information is represented as a nested hash table containing relations as keys, and associated values. For example, `DB['alexander hamilton'] = {'party': 'federalist',\n",
    " 'spouse': 'elizabeth schuyler', ...}`\n",
    "- `VOCAB`: A vocabulary object that maps words to tokens and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "PYzcdAR2ntvm"
   },
   "outputs": [],
   "source": [
    "# !git clone https://github.com/rlebret/wikipedia-biography-dataset.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "RZEHbtftn17L"
   },
   "outputs": [],
   "source": [
    "# !cat wikipedia-biography-dataset/wikipedia-biography-dataset.z?? > tmp.zip\n",
    "# !unzip -o tmp.zip\n",
    "# !rm tmp.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1rRYqmfSTcw0"
   },
   "source": [
    "Get all the wikipedia titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "mdVrlCf-4TAq"
   },
   "outputs": [],
   "source": [
    "train_titles = []\n",
    "with open(\"wikipedia-biography-dataset/train/train.title\", \"r\") as file:\n",
    "  for line in file:\n",
    "    train_titles.append(line.rstrip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YOkCic0NTeuS"
   },
   "source": [
    "Boxes contain all the information, with each line corresponding to a title in `titles`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "C80pu63x4o-Y"
   },
   "outputs": [],
   "source": [
    "train_boxes = []\n",
    "with open(\"wikipedia-biography-dataset/train/train.box\", \"r\") as file:\n",
    "  for line in file:\n",
    "    train_boxes.append(line.rstrip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "63cxS1DWTk0h"
   },
   "source": [
    "This will make the DB object, a dictionary of dictionaries for each wikipedia title, which is more or less the same as names. This function only keeps politicians (containing the \"office\" key term) and strips out information about images. It can be improved in many ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "Pr97KJLc4v7v"
   },
   "outputs": [],
   "source": [
    "# Make a dictionary of dictionaries\n",
    "def make_db(titles, boxes):\n",
    "  db = {} # The DB\n",
    "  # Iterate through titles\n",
    "  for i in tqdm(range(len(titles))):\n",
    "    box = boxes[i] # Grab the corresponding box information\n",
    "    d  = {} # Inner dictionary\n",
    "    # Build a dict for the ith entry\n",
    "    # grab each key:value pair\n",
    "    for pair in re.findall(r'([a-zA-Z_]+)[0-9]*\\:([\\w\\d]+)', box):\n",
    "      key, value = pair\n",
    "      # Do a bit of cleaning\n",
    "      key = key.strip()\n",
    "      value = value.strip()\n",
    "      # If the key contains the word image, we probably don't want to keep it\n",
    "      if 'image' not in key:\n",
    "        # The regex maintains underscores, strip those off\n",
    "        if key[-1] == '_':\n",
    "          key = key[:-1]\n",
    "        # Make a new entry in inner dictionary if we don't have one\n",
    "        if key not in d:\n",
    "          d[key] = value\n",
    "        # Keys with compound values are split up, which is annoying, so put them back together\n",
    "        else:\n",
    "          d[key] += ' ' + value\n",
    "    # If it has an office key, keep it.\n",
    "    if 'office' in d:\n",
    "      db[titles[i]] = d\n",
    "  return db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S9hB1o63U45e"
   },
   "source": [
    "Build the vocab from the DB. Convert the whole thing into a string, tokenize it, and feed the surviving words into the vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "p13O5dcVvdZu"
   },
   "outputs": [],
   "source": [
    "def make_vocab(DB):\n",
    "  # Make the vocab object\n",
    "  vocab = Vocab()\n",
    "  # Tokenize the data by converting the entire DB into a string\n",
    "  tokens = tokenize(str(DB))\n",
    "  # Iterate through all the tokens (tqdm provides a progress bar)\n",
    "  for t in tqdm(tokens):\n",
    "    vocab.add_word(t)\n",
    "  return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zkjf7mbAVA9H"
   },
   "source": [
    "If you want to discard rare words, this will rebuild the vocab. This is just an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "448Ww-BFtBL3"
   },
   "outputs": [],
   "source": [
    "def reduce_vocab(vocab, min_word_occurrence = 2):\n",
    "  # make a new vocab\n",
    "  vocab2 = Vocab(\"top\")\n",
    "  # Add the UNK token\n",
    "  vocab2.add_word(UNK)\n",
    "  # Iterate through vocabulary\n",
    "  for w in list(vocab._word2count.keys()):\n",
    "    count = vocab._word2count[w]\n",
    "    idx = vocab._word2index[w]\n",
    "    # If the word count passes threshold, add it to the new vocabulary object\n",
    "    if count >= min_word_occurrence:\n",
    "      vocab2.add_word(w)\n",
    "      vocab2._word2count[w] = count\n",
    "  # Return the new vocabulary object\n",
    "  return vocab2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iVS602YPVE9N"
   },
   "source": [
    "Make the DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "cio_ZwncrGf0"
   },
   "outputs": [],
   "source": [
    "# DB = make_db(train_titles, train_boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ETLqtWtyVKLE"
   },
   "source": [
    "Make the VOCAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "lc7oi3p54SOb"
   },
   "outputs": [],
   "source": [
    "# VOCAB = make_vocab(DB)\n",
    "# # Add UNK token to vocabulary\n",
    "# VOCAB.add_word(UNK)\n",
    "# print(VOCAB.num_words())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y9AP3Dcle5VC"
   },
   "source": [
    "## Save Processed Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QiTQxguSXEuq"
   },
   "source": [
    "You may find it useful to save the processed dataset to your Google Drive.\n",
    "\n",
    "It is recommended that you save the file to your Google Drive. To mount your Google Drive, open the file icon on the left side of the screen to get to the option). To save the file in your Google Drive use the path `'drive/MyDrive/filename'`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "6DNxueU-gpz_"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# dump in containing directory\n",
    "local_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "DwagcDU_Ut2Q"
   },
   "outputs": [],
   "source": [
    "# with open(os.path.join(local_path, \"data\"), \"wb\") as f:\n",
    "#   pickle.dump(DB, f, protocol=None, fix_imports=True, buffer_callback=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "WS6a8EtX_h0F"
   },
   "outputs": [],
   "source": [
    "# with open(os.path.join(local_path, \"vocab\"), 'wb') as f:\n",
    "#   pickle.dump(VOCAB, f, protocol=None, fix_imports=True, buffer_callback=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lzlH6IKhe_Qa"
   },
   "source": [
    "## Load processed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2AlkMbibXYWg"
   },
   "source": [
    "If you have saved the processed data in your Google Drive, you can re-load it with these commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "ZCaiBhdvCa_C"
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(local_path, \"vocab\"), \"rb\") as f:\n",
    "  VOCAB = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "IinOyL_4C1Hx"
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(local_path, \"data\"), \"rb\") as f:\n",
    "  DB = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vcZcFUxrfCnB"
   },
   "source": [
    "## Data example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jKKoOuLCVYxd"
   },
   "source": [
    "Get to know your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "dbLkOyB2pp_M"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'alexander hamilton',\n",
       " 'office': '1st united states secretary of the treasury senior officer of the army delegate to the congress of the confederation from new york',\n",
       " 'president': 'george washington john adams',\n",
       " 'term_start': 'september 11 1789 december 14 1799 november 3 1788 november 4 1782',\n",
       " 'term_end': 'january 31 1795 june 15 1800 march 2 1789 june 21 1783',\n",
       " 'predecessor': 'position established george washington egbert benson seat established',\n",
       " 'successor': 'oliver wolcott jr james wilkinson seat abolished seat abolished',\n",
       " 'birth_date': '11 january 1755',\n",
       " 'birth_place': 'charlestown nevis british west indies',\n",
       " 'death_date': 'july 12 1804 aged 47 or 49',\n",
       " 'death_place': 'new york city new york u',\n",
       " 'party': 'federalist',\n",
       " 'spouse': 'elizabeth schuyler',\n",
       " 'children': 'philip angelica alexander james alexander john church william stephen eliza holly phil',\n",
       " 'alma_mater': 'kings college new york',\n",
       " 'religion': 'presbyterian episcopalian convert',\n",
       " 'signature': 'alexander hamilton signaturert',\n",
       " 'allegiance': 'flag_of_new_york _ 1778 svg 23px new york 1775 1777 united states 1795 23px 1777 1800',\n",
       " 'branch': 'flag_of_new_york _ 1778 svg 23px new york company of artillery united states 1777 23px continental army 25px united states army',\n",
       " 'serviceyears': '1775 1776 militia 1776 1781 1798 1800',\n",
       " 'rank': '23px',\n",
       " 'article_title': 'alexander hamilton'}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DB[\"alexander hamilton\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ITACHJTWfE5m"
   },
   "source": [
    "# Part B: Implement the Key-Value Memory Network (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xQqLE-fcI_43"
   },
   "source": [
    "This [paper](https://arxiv.org/abs/1606.03126) describes the key-value memory networks in detail, which is also sketched out below.\n",
    "\n",
    "A key-value memory network takes a natural language question. This question will be converted into a bag-of-words (i.e., a multihot) Call this $x$ and it is a 1D tensor of vocabulary length.\n",
    "\n",
    "![KVMemNet architecture](https://github.com/markriedl/kvmemnet-assignment/blob/32479dd1e88a9f8dfc72f11ccb8e9e0e1f78905f/kvmemnet-inside.png?raw=true)\n",
    "\n",
    "The KVMemNet will contain a linear layer (or embedding layer) that will produce a 1D embedding of the question $q=A(x)$.\n",
    "\n",
    "The KVMemNet will also take in a stack of keys as a tensor of shape `num_keys x vocab_size`. Each row is embedded using the same embedding, $k=A(keys)$, producing a tensor of shape `num_keys x embed_dim`. How this stack of keys is chosen will be discussed below.\n",
    "\n",
    "The KVMemNet will take in a third input, a stack of values associated with the stack of keys. This will also be of shape `num_values x vocab_size`. Each row is embedded using the same embedding, $v=A(values)$, producing a tensor of shape `num_keys x embed_dim`.\n",
    "\n",
    "The KVMemNet will also contain a second linear embedding layer, $B$. More on this later.\n",
    "\n",
    "Once we have `q`, `k`, and `v` embeddings, the next step is to use `q` and `k` to compute attention scores that can be applied against `v`. Think of $A$ as learning how to make questions and the keys that should match against values that have received the same treatment.\n",
    "\n",
    "The attention scores `p` are computed by taking the inner-product (`torch.inner()`) between `q` and `k`. The result will be a 1D tensor with `num_keys` length. Use softmax so that `p` contains scores between 1.0 and 0.0.\n",
    "\n",
    "You may be wondering why there isn't a non-linearity such as a sigmoid or ReLU after the linear layer. Softmax is a non-linearity.\n",
    "\n",
    "Next apply the `p` attention scores against `v` to apply a weight against each value in the stack of values. One should be highly weighted and the rest less weighted. Sum all the weighted values up to create a 1D tensor `o` of feature weights of length `embed_dim`. `p` can be thought of as how much of each value gets selected. Then they all get combined together and the feature weights are proportional to how much each value was attended to. The `torch.matmul()` can do the multiplication and summing in one step.\n",
    "\n",
    "The KVMemNet forward function should return this tensor of feature weights `o`.\n",
    "\n",
    "A quick note on `k` and `v`. We can't send the entire set of keys and values in our database through the network's forward function. Instead there should be a selection mechanism that selects just a subset of the database. This subset should contain the best key for the question $x$ to match against, and its corresponding value. We assume that a shallow selection process can narrow down the key-value pairs to a relatively small set, one of which will be best. For example, if the question involves \"Alexander Hamilton\", we can reasonably guess that the best key-value pair is in the part of the database associated with the named person.\n",
    "\n",
    "We are not done though. What about our linear layer $B$? Suppose variable `Y` contains our entire set of values in our databse as bags of words. $B$ is going to be used to embed our entire set of database values $y=B(Y)$. $B$ can be thought of as learning how to make all the values look like the feature weights output by the model such that the highest cosine similarity corresponds to the correct value taken from *all* values in the database.\n",
    "\n",
    "$B$ should live inside the KVMemNet object so that its parameters become trainable, but notice that we do not use $B$ in the KVMemNet's forward function. $B$ will get used to prepare the stack of all values in the database for training. It will bet used in the training loop but outside of the forward function. This is a bit unusual, but necessary to figure out the correct target (the true index of the best value to match against) for training.\n",
    "\n",
    "The above explantion only implements *single-hop* retrieval. *multi-hop* retrieval allows the results of one retrieval to inform a second (and third and so on) to get the right retrieval. This would be used in the case where the answer cannot be inferred directly from the question in a single retrieval, such as \"What was the founding date of the country that Alexander Hamilton was born in?\". To implement multi-hop retrieval, the KVMemNet will have additional linear layers $R_1...R_n$. Each $R_{i}$ will do a linear transform on `q` then attention will score and retrieve values as feature weights `o`. This will be sent to the next $R_{i+1}$ and so on until the hops are complete. This final `o` will be returned.\n",
    "\n",
    "For this assignment is is sufficient to only do *single-hop* retrieval.\n",
    "\n",
    "The above explanation does not include consideration of batching. You may want to add a batch dimension as the first dimension and input a batch as a set of questions, a set of stacks of keys, and a set of stacks of values. To do this, functions like `.inner()`, `.mm()`, and`.matmul()` will not work. Instead use `.bmm()` which handles batching correctly. You will probably need to do some `.squeeze()` and `.unsqueeze()` operations to make sure your tensors are the correct shapes.\n",
    "\n",
    "Instead of bag-of-words, one may also consider first converting each question, key, and value into a general set of embeddings such as [GLoVe](https://nlp.stanford.edu/projects/glove/). To do this one will need to consider how to combine words--convert each word into an embedding vector and then add the vectors together (or maybe average them)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DSf1SSX7bgdE"
   },
   "source": [
    "**Complete the key-value memory net code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9zex3XZzw0-l"
   },
   "outputs": [],
   "source": [
    "class KVMemNet(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super(KVMemNet, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        ### YOUR CODE HERE\n",
    "        self.A = nn.Linear(vocab_size, embed_dim)\n",
    "        self.B = nn.Linear(vocab_size, embed_dim)\n",
    "\n",
    "    def forward(self, x, keys, values):\n",
    "        # pseudocode from OH:\n",
    "        # questions = A(x)\n",
    "        # keys = A(keys) \n",
    "        # values = A(values)\n",
    "        \n",
    "        # prob = questions * keys\n",
    "        # prob = softmax(prob)\n",
    "        # output = prob * values\n",
    "        \n",
    "        # Handle both single samples and batches\n",
    "        if x.dim() == 1:\n",
    "            x = x.unsqueeze(0)\n",
    "            keys = keys.unsqueeze(0) \n",
    "            values = values.unsqueeze(0)\n",
    "            single_sample = True\n",
    "        else:\n",
    "            single_sample = False\n",
    "\n",
    "        # Embed using layer A\n",
    "        questions = self.A(x)  # [batch, embed_dim]\n",
    "        keys_embed = self.A(keys)  # [batch, num_keys, embed_dim]\n",
    "        values_embed = self.A(values)  # [batch, num_keys, embed_dim]\n",
    "        \n",
    "        # Compute attention scores\n",
    "        # [batch, num_keys, embed_dim] x [batch, embed_dim, 1] = [batch, num_keys, 1]\n",
    "        prob = torch.bmm(keys_embed, questions.unsqueeze(2))  \n",
    "        prob = prob.squeeze(2)  # [batch, num_keys]\n",
    "        \n",
    "        # Apply softmax\n",
    "        prob = F.softmax(prob, dim=1)  # [batch, num_keys]\n",
    "        \n",
    "        # Weight values by attention and sum\n",
    "        # [batch, 1, num_keys] x [batch, num_keys, embed_dim] = [batch, 1, embed_dim]\n",
    "        output = torch.bmm(prob.unsqueeze(1), values_embed)\n",
    "        output = output.squeeze(1)  # [batch, embed_dim]\n",
    "        \n",
    "        if single_sample:\n",
    "            output = output.squeeze(0)\n",
    "            \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p8pshjz9hRps"
   },
   "source": [
    "\n",
    "# Synthetic Data Set\n",
    "\n",
    "This is a synthetic dataset. One way to test a model during development is to take a small piece of data and show that you can overfit a model. If you can't overfit an easily learned chunk of data, then you probably have something wrong in your code. In this case I have provided a small chunk of synthetic data that should be easy to learn.\n",
    "\n",
    "- The vocabulary is 20 word: 5 names, 5 relations, 5 question-words, 5 values\n",
    " - First 5 elements of the vocab are names (for example index 0 might be \"Hamilton\").\n",
    " - Second 5 elements of the vocab are relations (for example, \"born\", \"died\", \"occupation\").\n",
    " - Third 5 elements are random words that might be part of a query (for example, \"When was\").\n",
    " - Final 5 elements of the vocab are possible values (for example, \"1757\")\n",
    "- A \"question\" is a name (5, 1), relation (5, 1), some words (5, 1), and no values\n",
    "- The keys will all have the same name (5, 5) where each row is idential, relations (5, 5), no words, no values\n",
    "- Values will have no names, no relations, no words, and value vocab words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "Qvuyp-g7grYw"
   },
   "outputs": [],
   "source": [
    "# Turn on a different relation on each row\n",
    "relations = torch.zeros(5, 5)\n",
    "relations.fill_diagonal_(1)\n",
    "\n",
    "# training data\n",
    "train_data = {}\n",
    "for i in range(5):\n",
    "  # Name associated with questions, keys, values\n",
    "  train_data[i] = (torch.cat([F.one_hot(torch.arange(0, 5))[i].repeat(5, 1),\n",
    "                         relations,\n",
    "                         torch.randint(0, 2, (5, 5)).float(),\n",
    "                         torch.zeros(5, 5)], dim=1),\n",
    "              torch.cat([F.one_hot(torch.arange(0, 5))[i].repeat(5, 1),\n",
    "                         relations,\n",
    "                         torch.zeros(5, 5),\n",
    "                         torch.zeros(5, 5)], dim=1),\n",
    "              torch.cat([torch.zeros(5, 5),\n",
    "                         torch.zeros(5, 5),\n",
    "                         torch.zeros(5, 5),\n",
    "                         torch.randint(0, 2, (5, 5)).float()], dim=1))\n",
    "  Y = torch.cat([v[2] for v in list(train_data.values())], dim=0).to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G6R_xIi_TZVp"
   },
   "source": [
    "# Part C: Train on Synthetic Data (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4slHUG5cPDtT"
   },
   "source": [
    "The following describes the steps to set up a training loop, including the training of the $B$ layer.\n",
    "\n",
    "![The KVMemNet being used in the training loop](https://github.com/markriedl/kvmemnet-assignment/blob/main/kvmemnet-outside.png?raw=true)\n",
    "\n",
    "- Create a model with the given vocabulary size and an embedding size that is equal to or smaller.\n",
    "- Loop through `N` epochs:\n",
    " - There are five names, loop through each name.\n",
    "   - Get a stack of questions, stack of keys, and stack of values from `DB_synth`.\n",
    "   - Loop through the relations. There is relation on each row of the keys and values.\n",
    "     - Get a single question, the `i`-th row in the questions pulled from `DB_synth` above.\n",
    "     - Compute the target: this is the `name*5 + i` element in `Y`.\n",
    "     - Run the singular question, stack of keys, and stack of values through the model and produce an output, which is a tensor of feature weights.\n",
    "     - Run all of `Y` through `model.B()` to get an embedded stack of values.\n",
    "     - Take the softmax of the inner product between the embedded stack of values from `Y` and the feature weight generated by the model.\n",
    "     - Compute the loss with `nn.CrossEntropyLoss`.\n",
    "     - Call `.backward()` on the loss.\n",
    "\n",
    "In addition to printing the loss (after every question or after every name in `DB_synth`) you can also print the target and the argmax of the softmax result to see if they match. Over time you should see the target and the argmax in agreement. For the purposes of this part of the project it is sufficient to test on the training set.\n",
    "\n",
    "Don't forget to move the model and the tensor to the GPU.\n",
    "\n",
    "You may want to speed up training by implementing batching. To do this, the model `forward()` needs to take tensors with an extra batching dimension as the first dimension. However, `.inner()`, `.mm()`, and `.matmul()` will not work properly. You will need to use `.bmm()` instead, which understands the first dimension is for batching. You will likely find that you need to perform some `.squeeze()` and `.unsqueeze()` operations. You can try batch-size of one, or take entire chunks (or even all synthetic data as a single, large batch). Try it different ways.\n",
    "\n",
    "Try training on the synthetic data first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zu1CMkabZkyU"
   },
   "source": [
    "You may make as many cells as necessary. Save your notebook outputs that plot loss and show it reducing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0MqIdFVAbqr1"
   },
   "source": [
    "**Write code blocks below that create the `KVMemNet`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wxso4hXaTKSK"
   },
   "outputs": [],
   "source": [
    "# Pseudo from OH\n",
    "\"\"\"\n",
    "vocab_size = 20\n",
    "embed_size = 16\n",
    "model = KVMemNet(vocab_size, embed_size)\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\"\"\"\n",
    "\n",
    "vocab_size = 20\n",
    "embed_dim = 16\n",
    "learning_rate = 0.001\n",
    "num_epochs = 100  # Increased for better convergence\n",
    "\n",
    "model = KVMemNet(vocab_size, embed_dim).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "001D35pTb82K"
   },
   "source": [
    "**Write and run a training testing loop. Show that your training loop loss converges with a plot**\n",
    "\n",
    "To plot a loss curve, compute the mean loss per epoch and save it in a list:\n",
    "```\n",
    "x_axis.append(epoch_number)\n",
    "y_axis.append(mean_epoch_loss_for_this_epoch)\n",
    "plt.plot(x_axis, y_axis)\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "InBysHJfTTYX"
   },
   "outputs": [],
   "source": [
    "# Write your training loop here\n",
    "Y_tensor = Y.to(device)\n",
    "\n",
    "# Initialize loss tracking lists for plotting\n",
    "losses = []  # Loss per batch/sample\n",
    "epoch_losses = []  # Average loss per epoch\n",
    "\n",
    "# Pseudo from OH\n",
    "\"\"\"\n",
    "for epoch in range(500):\n",
    "    for name_index in range(5):\n",
    "        qs, keys, values = train_data[name_index]\n",
    "        keys = keys.to(device)\n",
    "        values = values.to(device)\n",
    "        \n",
    "        for relation_index in range(5):\n",
    "            q = qs[relation_index].to(device)\n",
    "            output = model(q, keys, values)\n",
    "            y = model.B(Y)\n",
    "            \n",
    "            soft = softmax(inner(y, output))\n",
    "            target_index = name_index * 5 + relation_index\n",
    "            \n",
    "            soft.unsqueeze if necessary\n",
    "            loss = criterion(soft, target)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            print('prediction', soft.argmax(), 'target', target_index, 'loss', loss.item())\n",
    "\"\"\"\n",
    "\n",
    "# Training loop based on OH pseudocode\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Loop through 5 names\n",
    "    for name_index in range(5):\n",
    "        # Get questions, keys, values for this person\n",
    "        qs, keys, values = train_data[name_index]\n",
    "        \n",
    "        # Move to device\n",
    "        qs = qs.to(device)\n",
    "        keys = keys.to(device)\n",
    "        values = values.to(device)\n",
    "        \n",
    "        # Process each relation/question for this person\n",
    "        for relation_index in range(5):\n",
    "            # Get single question\n",
    "            q = qs[relation_index]  # Shape: [20]\n",
    "            \n",
    "            # Forward pass through model\n",
    "            output = model(q, keys, values)  # Shape: [embed_dim]\n",
    "            \n",
    "            # Embed all Y values using layer B\n",
    "            y = model.B(Y_tensor)  # Shape: [25, embed_dim]\n",
    "            \n",
    "            # Compute inner product and softmax\n",
    "            scores = torch.inner(y, output)  # Shape: [25]\n",
    "            \n",
    "            # Target index for this name-relation combination\n",
    "            target_index = name_index * 5 + relation_index\n",
    "            target = torch.tensor([target_index], dtype=torch.long, device=device)\n",
    "            \n",
    "            # Add batch dimension for CrossEntropyLoss\n",
    "            scores = scores.unsqueeze(0)  # Shape: [1, 25]\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(scores, target)\n",
    "            \n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Track metrics\n",
    "            prediction = scores.argmax(dim=1).item()\n",
    "            correct += (prediction == target_index)\n",
    "            total += 1\n",
    "            \n",
    "            loss_value = loss.item()\n",
    "            losses.append(loss_value)\n",
    "            epoch_loss += loss_value\n",
    "            \n",
    "            # Print progress occasionally\n",
    "            if epoch % 20 == 0 and name_index == 0 and relation_index == 0:\n",
    "                print(f'Epoch {epoch}, prediction: {prediction}, target: {target_index}, loss: {loss_value:.4f}')\n",
    "    \n",
    "    # Track epoch average\n",
    "    avg_loss = epoch_loss / 25\n",
    "    epoch_losses.append(avg_loss)\n",
    "    \n",
    "    # Print epoch summary\n",
    "    if epoch % 20 == 0:\n",
    "        accuracy = 100.0 * correct / total\n",
    "        print(f\"Epoch [{epoch}/{num_epochs}] - Avg Loss: {avg_loss:.4f}, Accuracy: {accuracy:.1f}%\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(losses[-500:])  # Show last 500 samples\n",
    "plt.title('Training Loss (Last 500 Samples)')\n",
    "plt.xlabel('Sample')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epoch_losses)\n",
    "plt.title('Average Loss per Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Average Loss')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final average loss: {epoch_losses[-1]:.4f}\")\n",
    "print(f\"Final accuracy: {100.0 * correct / total:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nupd-Gr9P6vc"
   },
   "source": [
    "# Part D: Training on the Full Data (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r02Y9yvhPXy8"
   },
   "source": [
    "To train on the full data, you are going to need to do some pre-processing of the data.\n",
    "\n",
    "First, there are no \"questions\". You need to generate questions for each type of relation. There a number of ways to do this. The simplest is to just assume that a question is the name of a person and a relation, e.g., \"Alexander Hamilton birth date\". Another way would be to create templates for each type of relation. For example the \"birth date\" relation would have the following template: \"When was [name] born?\", filling in the [name]. Because there are a lot of different types of relations, you may want to remove the more obscure relations so you need fewer templates and also have a smaller vocabulary. Templates work well if the questions are expected to be almost identical to the templates. You may want to generate multiple templates per relation. Continuing the previous example, a second template would be: \"What is the birthdate of [name]?\".\n",
    "\n",
    "If you are feeling more ambitious, you could use GPT-J, GPT-NeoX, GPT-3 or ChatGPT to generate templates. It works decently well and you can get some variety of templates.\n",
    "\n",
    "The question should contain information about the person and some words that are representative of the relation even if the exact relation words aren't used (the KVMemNet should figure out that \"birthdate\" and \"born\" are correlated).\n",
    "\n",
    "You only put a subset of all key-value pairs into the KVMemNet. You need a technique for sub-selecting from all the key-value pairs in `DB`. You might just need the ones that are directly associated with the person (Alexander Hamilton has 23). You may need to mix in a few key-value pairs from another person's entries in the database to help ensure against accidental overfitting.\n",
    "\n",
    "The final challenge you will have in the training loop is that there may still be too many unique values in `Y` to encode and create one big tensor. In that case, you can at least use the values that you sent to the KVMemNet, along with as many other randomly selected values as you can fit into the GPU's memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3I3CTqeEfa1m"
   },
   "source": [
    "Create as many cells below as you need. Save the output of your training and testing functions, reporting loss during training and accuracy during testing. 5 points for a training loop that reduces loss. 5 points for a training function with a correct accuracy computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D-970kRwZ26n"
   },
   "source": [
    "**Create a training dataset and a non-overlapping testing dataset**\n",
    "\n",
    "If CPU memory becomes a problem you might want to consider a `DataLoader` so that data can be stored on file and pulled up when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "V6VEnB_VS7Px"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating training and test datasets...\n",
      "Train names: 32628, Test names: 8158\n",
      "Created dataset with 32628 persons and 154746 total samples\n",
      "Created dataset with 8158 persons and 38783 total samples\n",
      "Creating Y matrix with 5000 unique values...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building Y matrix: 100%|██████████| 10/10 [00:00<00:00, 24.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y matrix created with shape: torch.Size([5000, 96094])\n",
      "Dataset creation completed!\n",
      "Training samples: 154746\n",
      "Test samples: 38783\n",
      "Y matrix shape: torch.Size([5000, 96094])\n"
     ]
    }
   ],
   "source": [
    "# Create your training and test sets here\n",
    "import torch.utils.data as data\n",
    "\n",
    "# Simplified question templates to avoid too many OOV words\n",
    "relation_templates = {\n",
    "    'birth': ['{name} birth', 'when {name} born', '{name} birth date'],\n",
    "    'birthplace': ['{name} birthplace', 'where {name} born', '{name} birth place'],\n",
    "    'death': ['{name} death', 'when {name} died', '{name} death date'],\n",
    "    'deathplace': ['{name} deathplace', 'where {name} died', '{name} death place'],\n",
    "    'office': ['{name} office', '{name} position', '{name} job'],\n",
    "    'party': ['{name} party', '{name} political party'],\n",
    "    'spouse': ['{name} spouse', '{name} married', '{name} wife'],\n",
    "    'successor': ['{name} successor', 'who succeeded {name}'],\n",
    "    'predecessor': ['{name} predecessor', 'who preceded {name}'],\n",
    "    'education': ['{name} education', '{name} school', '{name} studied'],\n",
    "    'occupation': ['{name} occupation', '{name} job', '{name} work'],\n",
    "    'religion': ['{name} religion', '{name} religious'],\n",
    "    'nationality': ['{name} nationality', '{name} country'],\n",
    "    'residence': ['{name} residence', '{name} lived', '{name} home'],\n",
    "    'alma_mater': ['{name} alma mater', '{name} graduated']\n",
    "}\n",
    "\n",
    "def create_question_from_template(name, relation):\n",
    "    \"\"\"Create a question using templates for a given name and relation\"\"\"\n",
    "    if relation in relation_templates:\n",
    "        template = random.choice(relation_templates[relation])\n",
    "        return template.format(name=name)\n",
    "    else:\n",
    "        # Default template for unknown relations\n",
    "        return f\"{name} {relation}\"\n",
    "\n",
    "def multihot_torch_optimized(s, vocab):\n",
    "    \"\"\"\n",
    "    Optimized PyTorch-based multihot encoding that's much faster than NumPy version\n",
    "    Creates tensors directly without NumPy intermediate steps\n",
    "    \"\"\"\n",
    "    tokens = tokenize(s)\n",
    "    \n",
    "    # Build list of valid indices\n",
    "    indices = []\n",
    "    for t in tokens:\n",
    "        if t in vocab._word2index:\n",
    "            indices.append(vocab.word2index(t))\n",
    "        elif UNK in vocab._word2index:\n",
    "            indices.append(vocab.word2index(UNK))\n",
    "    \n",
    "    if not indices:\n",
    "        return torch.zeros(vocab.num_words(), dtype=torch.float32)\n",
    "    \n",
    "    # Create tensor directly with PyTorch operations\n",
    "    mhot = torch.zeros(vocab.num_words(), dtype=torch.float32)\n",
    "    indices_tensor = torch.tensor(indices, dtype=torch.long)\n",
    "    mhot.index_fill_(0, indices_tensor, 1.0)\n",
    "    \n",
    "    return mhot\n",
    "\n",
    "class KVMemNetDatasetPersonBased(data.Dataset):\n",
    "    \"\"\"\n",
    "    Dataset that properly groups data by person, then samples relations\n",
    "    Each sample is still a (question, keys, values) tuple for a specific relation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, names, DB, vocab, relation_templates, max_key_pairs=10):\n",
    "        self.DB = DB\n",
    "        self.vocab = vocab\n",
    "        self.relation_templates = relation_templates\n",
    "        self.max_key_pairs = max_key_pairs\n",
    "        \n",
    "        # Build person-based structure\n",
    "        self.persons = []  # List of (name, valid_relations) tuples\n",
    "        self.samples = []  # Flattened list of (person_idx, relation) for training\n",
    "        \n",
    "        # Process each person\n",
    "        for person_idx, name in enumerate(names):\n",
    "            if name in DB:\n",
    "                person_data = DB[name]\n",
    "                valid_relations = [r for r in person_data.keys() if r in relation_templates]\n",
    "                \n",
    "                if valid_relations:\n",
    "                    self.persons.append((name, valid_relations))\n",
    "                    # Create samples for each relation of this person\n",
    "                    for relation in valid_relations:\n",
    "                        self.samples.append((len(self.persons) - 1, relation))\n",
    "        \n",
    "        print(f\"Created dataset with {len(self.persons)} persons and {len(self.samples)} total samples\")\n",
    "        \n",
    "        # Pre-compute list of person indices for random sampling\n",
    "        self.person_indices = list(range(len(self.persons)))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get a sample for training. Each sample is one question about one relation\n",
    "        for a specific person, but includes all their relations as keys/values\n",
    "        \"\"\"\n",
    "        person_idx, target_relation = self.samples[idx]\n",
    "        person_name, person_relations = self.persons[person_idx]\n",
    "        person_data = self.DB[person_name]\n",
    "        \n",
    "        # Create question for the target relation\n",
    "        question_text = create_question_from_template(person_name, target_relation)\n",
    "        question_tensor = multihot_torch_optimized(question_text, self.vocab)\n",
    "        \n",
    "        # Prepare keys and values\n",
    "        keys = []\n",
    "        values = []\n",
    "        \n",
    "        # Add ALL relations for this person (target first)\n",
    "        # First add the target relation\n",
    "        target_key = f\"{person_name} {target_relation}\"\n",
    "        target_value = person_data[target_relation]\n",
    "        keys.append(multihot_torch_optimized(target_key, self.vocab))\n",
    "        values.append(multihot_torch_optimized(target_value, self.vocab))\n",
    "        \n",
    "        # Then add other relations from the same person\n",
    "        for rel in person_relations:\n",
    "            if rel != target_relation:\n",
    "                key_text = f\"{person_name} {rel}\"\n",
    "                value_text = person_data[rel]\n",
    "                keys.append(multihot_torch_optimized(key_text, self.vocab))\n",
    "                values.append(multihot_torch_optimized(value_text, self.vocab))\n",
    "        \n",
    "        # If we have fewer than max_key_pairs, add some from other people\n",
    "        if len(keys) < self.max_key_pairs:\n",
    "            # Use deterministic random sampling based on idx\n",
    "            rng = np.random.RandomState(idx)\n",
    "            other_person_indices = [i for i in self.person_indices if i != person_idx]\n",
    "            \n",
    "            if other_person_indices:\n",
    "                # Shuffle and select other persons\n",
    "                rng.shuffle(other_person_indices)\n",
    "                \n",
    "                for other_idx in other_person_indices:\n",
    "                    if len(keys) >= self.max_key_pairs:\n",
    "                        break\n",
    "                    \n",
    "                    other_name, other_relations = self.persons[other_idx]\n",
    "                    if other_relations:\n",
    "                        # Pick a random relation from this other person\n",
    "                        random_rel = rng.choice(other_relations)\n",
    "                        random_key = f\"{other_name} {random_rel}\"\n",
    "                        random_value = self.DB[other_name][random_rel]\n",
    "                        \n",
    "                        keys.append(multihot_torch_optimized(random_key, self.vocab))\n",
    "                        values.append(multihot_torch_optimized(random_value, self.vocab))\n",
    "        \n",
    "        # Ensure we have exactly max_key_pairs (truncate if necessary)\n",
    "        keys = keys[:self.max_key_pairs]\n",
    "        values = values[:self.max_key_pairs]\n",
    "        \n",
    "        # Pad with zeros if still not enough\n",
    "        while len(keys) < self.max_key_pairs:\n",
    "            keys.append(torch.zeros(self.vocab.num_words(), dtype=torch.float32))\n",
    "            values.append(torch.zeros(self.vocab.num_words(), dtype=torch.float32))\n",
    "        \n",
    "        # Stack tensors\n",
    "        keys_tensor = torch.stack(keys)\n",
    "        values_tensor = torch.stack(values)\n",
    "        \n",
    "        return {\n",
    "            'question': question_tensor,\n",
    "            'keys': keys_tensor,\n",
    "            'values': values_tensor,\n",
    "            'answer_text': target_value,\n",
    "        }\n",
    "\n",
    "def create_value_matrix_efficient(DB, vocab, max_values=5000):\n",
    "    \"\"\"Create a matrix of unique values more efficiently\"\"\"\n",
    "    value_list = []\n",
    "    value_to_idx = {}\n",
    "    \n",
    "    # Collect unique values\n",
    "    for name, person_data in DB.items():\n",
    "        for relation, value in person_data.items():\n",
    "            if relation in relation_templates and len(value_list) < max_values:\n",
    "                if value not in value_to_idx:  # Check using dict for O(1) lookup\n",
    "                    value_to_idx[value] = len(value_list)\n",
    "                    value_list.append(value)\n",
    "    \n",
    "    # Convert to tensor matrix using optimized function\n",
    "    print(f\"Creating Y matrix with {len(value_list)} unique values...\")\n",
    "    Y_list = []\n",
    "    \n",
    "    # Process in batches to show progress\n",
    "    batch_size = 500\n",
    "    for i in tqdm(range(0, len(value_list), batch_size), desc=\"Building Y matrix\"):\n",
    "        batch_end = min(i + batch_size, len(value_list))\n",
    "        for j in range(i, batch_end):\n",
    "            Y_list.append(multihot_torch_optimized(value_list[j], vocab))\n",
    "    \n",
    "    Y_matrix = torch.stack(Y_list)\n",
    "    print(f\"Y matrix created with shape: {Y_matrix.shape}\")\n",
    "    \n",
    "    return Y_matrix, value_list, value_to_idx\n",
    "\n",
    "# Create datasets\n",
    "print(\"Creating training and test datasets...\")\n",
    "\n",
    "# Split names into train/test\n",
    "all_names = list(DB.keys())\n",
    "random.seed(42)  # For reproducibility\n",
    "random.shuffle(all_names)\n",
    "split_idx = int(len(all_names) * 0.8)\n",
    "train_names = all_names[:split_idx]\n",
    "test_names = all_names[split_idx:]\n",
    "\n",
    "print(f\"Train names: {len(train_names)}, Test names: {len(test_names)}\")\n",
    "\n",
    "# Create person-based datasets\n",
    "train_dataset = KVMemNetDatasetPersonBased(train_names, DB, VOCAB, relation_templates)\n",
    "test_dataset = KVMemNetDatasetPersonBased(test_names, DB, VOCAB, relation_templates)\n",
    "\n",
    "# Create Y matrix efficiently\n",
    "Y_matrix, Y_values, value_to_idx = create_value_matrix_efficient(DB, VOCAB, max_values=5000)\n",
    "\n",
    "print(f\"Dataset creation completed!\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "print(f\"Y matrix shape: {Y_matrix.shape}\")\n",
    "\n",
    "# Create DataLoaders without multiprocessing to avoid memory issues\n",
    "# Set num_workers=0 to prevent memory duplication\n",
    "train_loader = data.DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=32, \n",
    "    shuffle=True, \n",
    "    num_workers=0,  # Critical: no multiprocessing to avoid memory leak\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = data.DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=64, \n",
    "    shuffle=False, \n",
    "    num_workers=0,  # Critical: no multiprocessing to avoid memory leak\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MI3b5x2zfl8g"
   },
   "source": [
    "**Create your `KVMemNet`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "OEKJ1KJxTBUJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real data model created with vocab_size=96094, embed_dim=128\n",
      "Model moved to device: cuda\n",
      "Optimizer: Adam with lr=0.001\n",
      "Loss function: CrossEntropyLoss\n"
     ]
    }
   ],
   "source": [
    "# Set up your KVMemNet here\n",
    "vocab_size = VOCAB.num_words()\n",
    "embed_dim = 128  # Larger embedding for real data\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Create model and move to GPU\n",
    "real_model = KVMemNet(vocab_size, embed_dim).to(device)\n",
    "\n",
    "# Setup optimizer and loss criterion\n",
    "real_optimizer = optim.Adam(real_model.parameters(), lr=learning_rate)\n",
    "real_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(f\"Real data model created with vocab_size={vocab_size}, embed_dim={embed_dim}\")\n",
    "print(f\"Model moved to device: {device}\")\n",
    "print(f\"Optimizer: Adam with lr={learning_rate}\")\n",
    "print(f\"Loss function: CrossEntropyLoss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Ug_8X-2fwYD"
   },
   "source": [
    "**Write and run a training loop, showing a loss plot**\n",
    "\n",
    "You may find it handy to also test your network on the test data periodically as it trains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, filename):\n",
    "    \"\"\"Save the model state dictionary to a file.\"\"\"\n",
    "    torch.save(model.state_dict(), filename)\n",
    "    print(f\"Model saved to {filename}\")\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, filename):\n",
    "    \"\"\"Save the model and optimizer state to a checkpoint file.\"\"\"\n",
    "    checkpoint = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'epoch': epoch\n",
    "    }\n",
    "    torch.save(checkpoint, filename)\n",
    "    print(f\"Checkpoint saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "-K6rxBHFTEPO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized with vocab_size=96094, embed_dim=128\n",
      "Starting training for 3 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:  20%|██        | 981/4836 [04:01<15:47,  4.07it/s, Loss=6.3018, Acc=4.90%]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[103]\u001b[39m\u001b[32m, line 104\u001b[39m\n\u001b[32m    101\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel initialized with vocab_size=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvocab_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, embed_dim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00membed_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    103\u001b[39m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m train_losses, train_epoch_losses = \u001b[43mtrain_model_simple\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreal_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_to_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreal_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal_criterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[38;5;66;03m# Plot training curves\u001b[39;00m\n\u001b[32m    110\u001b[39m plt.figure(figsize=(\u001b[32m12\u001b[39m, \u001b[32m4\u001b[39m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[103]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mtrain_model_simple\u001b[39m\u001b[34m(model, train_loader, Y_matrix, value_to_idx, optimizer, criterion, num_epochs, device)\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Progress bar for batches\u001b[39;00m\n\u001b[32m     23\u001b[39m progress = tqdm(train_loader, desc=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Get batch data\u001b[39;49;00m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquestions\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mquestion\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mkeys\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLP/hw5/.venv/lib/python3.13/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLP/hw5/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:729\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    728\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Any:\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprofiler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecord_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_profile_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    730\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sampler_iter\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m    731\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;49;00m\n\u001b[32m    732\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-arg]\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLP/hw5/.venv/lib/python3.13/site-packages/torch/autograd/profiler.py:776\u001b[39m, in \u001b[36mrecord_function.__exit__\u001b[39m\u001b[34m(self, exc_type, exc_value, traceback)\u001b[39m\n\u001b[32m    771\u001b[39m     \u001b[38;5;28mself\u001b[39m.record = torch.ops.profiler._record_function_enter_new(\n\u001b[32m    772\u001b[39m         \u001b[38;5;28mself\u001b[39m.name, \u001b[38;5;28mself\u001b[39m.args\n\u001b[32m    773\u001b[39m     )\n\u001b[32m    774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m776\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type: Any, exc_value: Any, traceback: Any):\n\u001b[32m    777\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.run_callbacks_on_exit:\n\u001b[32m    778\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Simplified training loop for real data following OH pattern\n",
    "def train_model_simple(model, train_loader, Y_matrix, value_to_idx, \n",
    "                      optimizer, criterion, num_epochs=5, device='cuda'):\n",
    "    \"\"\"\n",
    "    Simplified training loop that follows the OH pseudocode pattern\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    losses = []\n",
    "    epoch_losses = []\n",
    "    \n",
    "    # Move Y_matrix to device once\n",
    "    Y_matrix = Y_matrix.to(device)\n",
    "    \n",
    "    print(f\"Starting training for {num_epochs} epochs...\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        # Progress bar for batches\n",
    "        progress = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        for batch_idx, batch in enumerate(progress):\n",
    "            # Get batch data\n",
    "            questions = batch['question'].to(device)\n",
    "            keys = batch['keys'].to(device)\n",
    "            values = batch['values'].to(device)\n",
    "            answer_texts = batch['answer_text']\n",
    "            \n",
    "            # Find target indices\n",
    "            targets = []\n",
    "            valid_mask = []\n",
    "            \n",
    "            for i, answer_text in enumerate(answer_texts):\n",
    "                if answer_text in value_to_idx:\n",
    "                    targets.append(value_to_idx[answer_text])\n",
    "                    valid_mask.append(i)\n",
    "            \n",
    "            if not targets:\n",
    "                continue\n",
    "                \n",
    "            # Filter to valid samples\n",
    "            targets = torch.tensor(targets, dtype=torch.long, device=device)\n",
    "            questions = questions[valid_mask]\n",
    "            keys = keys[valid_mask]\n",
    "            values = values[valid_mask]\n",
    "            \n",
    "            # Forward pass (batched)\n",
    "            output = model(questions, keys, values)  # [batch, embed_dim]\n",
    "            \n",
    "            # Embed Y values\n",
    "            y_embedded = model.B(Y_matrix)  # [num_values, embed_dim]\n",
    "            \n",
    "            # Compute scores for all samples in batch\n",
    "            # [batch, embed_dim] x [num_values, embed_dim]^T = [batch, num_values]\n",
    "            scores = torch.matmul(output, y_embedded.T)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(scores, targets)\n",
    "            \n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Track metrics\n",
    "            predictions = scores.argmax(dim=1)\n",
    "            correct += (predictions == targets).sum().item()\n",
    "            total += len(targets)\n",
    "            \n",
    "            batch_loss = loss.item()\n",
    "            losses.append(batch_loss)\n",
    "            epoch_loss += batch_loss * len(targets)\n",
    "            \n",
    "            # Update progress bar\n",
    "            accuracy = 100.0 * correct / total if total > 0 else 0\n",
    "            progress.set_postfix({'Loss': f'{batch_loss:.4f}', 'Acc': f'{accuracy:.2f}%'})\n",
    "        \n",
    "        # Epoch statistics\n",
    "        avg_loss = epoch_loss / total if total > 0 else 0\n",
    "        epoch_accuracy = 100.0 * correct / total if total > 0 else 0\n",
    "        epoch_losses.append(avg_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: Loss = {avg_loss:.4f}, Accuracy = {epoch_accuracy:.2f}%\")\n",
    "    \n",
    "    return losses, epoch_losses\n",
    "\n",
    "# Create a simpler model configuration\n",
    "vocab_size = VOCAB.num_words()\n",
    "embed_dim = 128  # Reasonable size\n",
    "\n",
    "# Initialize model\n",
    "real_model = KVMemNet(vocab_size, embed_dim).to(device)\n",
    "\n",
    "# Simple optimizer\n",
    "real_optimizer = optim.Adam(real_model.parameters(), lr=0.001)\n",
    "real_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(f\"Model initialized with vocab_size={vocab_size}, embed_dim={embed_dim}\")\n",
    "\n",
    "# Train the model\n",
    "train_losses, train_epoch_losses = train_model_simple(\n",
    "    real_model, train_loader, Y_matrix, value_to_idx,\n",
    "    real_optimizer, real_criterion, num_epochs=3, device=device\n",
    ")\n",
    "\n",
    "# Plot training curves\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "# Show a subset of losses for clarity\n",
    "if len(train_losses) > 1000:\n",
    "    plt.plot(train_losses[::len(train_losses)//1000])\n",
    "else:\n",
    "    plt.plot(train_losses)\n",
    "plt.title('Training Loss Samples')\n",
    "plt.xlabel('Sample')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_epoch_losses, 'bo-')\n",
    "plt.title('Average Loss per Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Average Loss')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Training completed! Final average loss: {train_epoch_losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7g5uXKRAgDjD"
   },
   "source": [
    "**Write the code for testing your model on the test data**\n",
    "\n",
    "Your training loop can call the testing loop. But make sure that you do one last test on the model after training completes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ecGNHCNhTGxK"
   },
   "outputs": [],
   "source": [
    "# Simplified testing loop\n",
    "def evaluate_model_simple(model, test_loader, Y_matrix, value_to_idx, \n",
    "                         criterion, device='cuda'):\n",
    "    \"\"\"\n",
    "    Simple evaluation following the same pattern as training\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0\n",
    "    \n",
    "    # Move Y_matrix to device\n",
    "    Y_matrix = Y_matrix.to(device)\n",
    "    \n",
    "    print(\"Evaluating model on test set...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "            # Get batch data\n",
    "            questions = batch['question'].to(device)\n",
    "            keys = batch['keys'].to(device)\n",
    "            values = batch['values'].to(device)\n",
    "            answer_texts = batch['answer_text']\n",
    "            \n",
    "            # Find target indices\n",
    "            targets = []\n",
    "            valid_mask = []\n",
    "            \n",
    "            for i, answer_text in enumerate(answer_texts):\n",
    "                if answer_text in value_to_idx:\n",
    "                    targets.append(value_to_idx[answer_text])\n",
    "                    valid_mask.append(i)\n",
    "            \n",
    "            if not targets:\n",
    "                continue\n",
    "                \n",
    "            # Filter to valid samples\n",
    "            targets = torch.tensor(targets, dtype=torch.long, device=device)\n",
    "            questions = questions[valid_mask]\n",
    "            keys = keys[valid_mask]\n",
    "            values = values[valid_mask]\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(questions, keys, values)\n",
    "            \n",
    "            # Embed Y values\n",
    "            y_embedded = model.B(Y_matrix)\n",
    "            \n",
    "            # Compute scores\n",
    "            scores = torch.matmul(output, y_embedded.T)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(scores, targets)\n",
    "            total_loss += loss.item() * len(targets)\n",
    "            \n",
    "            # Compute accuracy\n",
    "            predictions = scores.argmax(dim=1)\n",
    "            correct += (predictions == targets).sum().item()\n",
    "            total += len(targets)\n",
    "    \n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    avg_loss = total_loss / total if total > 0 else 0\n",
    "    \n",
    "    print(f\"\\nTest Results:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f} ({correct}/{total})\")\n",
    "    print(f\"Average Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    return accuracy, avg_loss\n",
    "\n",
    "# Evaluate the model\n",
    "test_accuracy, test_loss = evaluate_model_simple(\n",
    "    real_model, test_loader, Y_matrix, value_to_idx, \n",
    "    real_criterion, device=device\n",
    ")\n",
    "\n",
    "# Show some example predictions\n",
    "def show_examples(model, test_dataset, Y_matrix, Y_values, num_examples=5):\n",
    "    \"\"\"Show a few example predictions\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    print(\"\\nExample predictions:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    indices = np.random.choice(len(test_dataset), num_examples, replace=False)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx in indices:\n",
    "            sample = test_dataset[idx]\n",
    "            \n",
    "            # Get data\n",
    "            question = sample['question'].unsqueeze(0).to(device)\n",
    "            keys = sample['keys'].unsqueeze(0).to(device)\n",
    "            values = sample['values'].unsqueeze(0).to(device)\n",
    "            answer_text = sample['answer_text']\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(question, keys, values)\n",
    "            \n",
    "            # Get scores\n",
    "            Y_matrix_gpu = Y_matrix.to(device)\n",
    "            y_embedded = model.B(Y_matrix_gpu)\n",
    "            scores = torch.matmul(y_embedded, output.squeeze())\n",
    "            \n",
    "            predicted_idx = torch.argmax(scores).item()\n",
    "            \n",
    "            if predicted_idx < len(Y_values):\n",
    "                predicted_text = Y_values[predicted_idx]\n",
    "            else:\n",
    "                predicted_text = \"Unknown\"\n",
    "            \n",
    "            print(f\"Actual: {answer_text}\")\n",
    "            print(f\"Predicted: {predicted_text}\")\n",
    "            print(f\"Match: {'✓' if answer_text == predicted_text else '✗'}\")\n",
    "            print(\"-\" * 60)\n",
    "\n",
    "show_examples(real_model, test_dataset, Y_matrix, Y_values, num_examples=5)\n",
    "\n",
    "print(f\"\\nFinal Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Final Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fSfTnmWbf0Pr"
   },
   "source": [
    "**Suggestion:** Once you have a model that has decent accuracy, you may want to save it to your Google Drive using ``torch.save()`` and load it when working on the next part of the assignment using ``torch.load()``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lRCn-LZmMNCp"
   },
   "source": [
    "# Part E: Use the Model (5 points)\n",
    "\n",
    "Given a question in natural language, turn it into a bag of words and feed it into the model with a set of plausible keys and values. Apply the output feature embedding to the full set of values and pick the value with argmax. Return the actual text inside that value (not the bag of words or embedding).\n",
    "\n",
    "That is, given a natural language question, you are asked to create the $q$ and pick a relevant subset of $k$ and $v$. Run the $q$, $k$, and $v$ through the model and get an answer to the original question.\n",
    "\n",
    "For example a question might be \"When was Alexander Hamilton born?\" Depending on how you pre-proessed your data, you may need to extract the entity and the relation.\n",
    "\n",
    "Write a function that takes in the `question` below, the data, and the model, and outputs the text answer, e.g., \"11 january 1755\". You must use your ``KVMemNet``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yiRYRlRhfFUG"
   },
   "source": [
    "**Suggestion:** To process a question you will probably want to find the entity and the relation. You may use packages such as [NLTK](https://www.nltk.org/) (already imported), [SpaCY](https://spacy.io/), [Stanza](https://stanfordnlp.github.io/stanza/), or other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ofPVuNzWeIto"
   },
   "source": [
    "Change the question to test your implementation, but don't delete this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JQLNvGsfm3b3"
   },
   "outputs": [],
   "source": [
    "question = \"When was alexander hamilton born?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KGSewc7jeaj1"
   },
   "source": [
    "**Create your function for using the `KVMemNet` to answer a given question.**\n",
    "\n",
    "The function should take in the question, data, model, and any other parameters you need. The function should return a text string.\n",
    "\n",
    "You can create as many cells as necessary. Save the notebook cells showing one example of your input question and output answer. For grading we will look to see that your question is in natural language, the model is used, and the answer is in text. The example doesn't have to be correct. You will analyze your technique later in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sN0ZNtADTzDd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing question answering function...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Y_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 182\u001b[39m\n\u001b[32m    180\u001b[39m \u001b[38;5;66;03m# Test with the provided question\u001b[39;00m\n\u001b[32m    181\u001b[39m test_question = \u001b[33m\"\u001b[39m\u001b[33mWhen was alexander hamilton born?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m answer = answer_question(test_question, real_model, DB, VOCAB, \u001b[43mY_matrix\u001b[49m)\n\u001b[32m    183\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mQuestion: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_question\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    184\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAnswer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00manswer\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'Y_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "# Create your functions here\n",
    "\n",
    "def extract_entity_and_relation(question):\n",
    "    \"\"\"\n",
    "    Extract the entity (person name) and relation type from a natural language question\n",
    "    \"\"\"\n",
    "    question_lower = question.lower()\n",
    "    \n",
    "    # Common question patterns and their corresponding relations\n",
    "    relation_patterns = {\n",
    "        'birth': ['born', 'birth', 'birthdate', 'birth date'],\n",
    "        'birthplace': ['birthplace', 'birth place', 'where.*born'],\n",
    "        'death': ['died', 'death', 'deathdate', 'death date'],\n",
    "        'deathplace': ['deathplace', 'death place', 'where.*died'],\n",
    "        'office': ['office', 'position', 'job', 'role'],\n",
    "        'party': ['party', 'political party'],\n",
    "        'spouse': ['married', 'spouse', 'wife', 'husband'],\n",
    "        'successor': ['successor', 'succeeded'],\n",
    "        'predecessor': ['predecessor', 'preceded'],\n",
    "        'education': ['education', 'school', 'university', 'studied'],\n",
    "        'occupation': ['occupation', 'profession', 'job', 'work'],\n",
    "        'religion': ['religion', 'religious'],\n",
    "        'nationality': ['nationality', 'country', 'from'],\n",
    "        'residence': ['residence', 'lived', 'home'],\n",
    "        'alma_mater': ['alma mater', 'graduated', 'degree']\n",
    "    }\n",
    "    \n",
    "    # Try to find relation based on keywords\n",
    "    detected_relation = None\n",
    "    for relation, patterns in relation_patterns.items():\n",
    "        for pattern in patterns:\n",
    "            if pattern in question_lower:\n",
    "                detected_relation = relation\n",
    "                break\n",
    "        if detected_relation:\n",
    "            break\n",
    "    \n",
    "    # Extract entity - look for capitalized words that might be names\n",
    "    tokens = question.split()\n",
    "    entity_candidates = []\n",
    "    \n",
    "    for i, token in enumerate(tokens):\n",
    "        # Look for capitalized words that are not at the beginning of sentences\n",
    "        if token[0].isupper() and token.lower() not in ['when', 'where', 'what', 'who', 'how', 'why']:\n",
    "            # Check if next token is also capitalized (likely part of name)\n",
    "            if i + 1 < len(tokens) and tokens[i + 1][0].isupper():\n",
    "                entity_candidates.append(f\"{token} {tokens[i + 1]}\")\n",
    "            else:\n",
    "                entity_candidates.append(token)\n",
    "    \n",
    "    # Take the first reasonable entity candidate\n",
    "    entity = entity_candidates[0].lower() if entity_candidates else \"\"\n",
    "    \n",
    "    return entity, detected_relation\n",
    "\n",
    "def get_relevant_keys_values(entity, relation, DB, max_pairs=10):\n",
    "    \"\"\"\n",
    "    Get relevant key-value pairs for a given entity and relation\n",
    "    \"\"\"\n",
    "    keys = []\n",
    "    values = []\n",
    "    target_idx = -1\n",
    "    \n",
    "    # Find the person in the database\n",
    "    person_data = None\n",
    "    for name, data in DB.items():\n",
    "        if entity in name.lower():\n",
    "            person_data = data\n",
    "            entity = name  # Use the exact name from DB\n",
    "            break\n",
    "    \n",
    "    if not person_data:\n",
    "        return keys, values, target_idx, None\n",
    "    \n",
    "    # Add the target key-value pair if relation exists\n",
    "    if relation and relation in person_data:\n",
    "        key_text = f\"{entity} {relation}\"\n",
    "        value_text = person_data[relation]\n",
    "        \n",
    "        keys.append(multihot(key_text, VOCAB))\n",
    "        values.append(multihot(value_text, VOCAB))\n",
    "        target_idx = 0\n",
    "    \n",
    "    # Add other relations for the same person\n",
    "    other_relations = [r for r in person_data.keys() if r != relation]\n",
    "    random.shuffle(other_relations)\n",
    "    \n",
    "    for other_rel in other_relations[:max_pairs-1]:\n",
    "        other_key_text = f\"{entity} {other_rel}\"\n",
    "        other_value_text = person_data[other_rel]\n",
    "        \n",
    "        keys.append(multihot(other_key_text, VOCAB))\n",
    "        values.append(multihot(other_value_text, VOCAB))\n",
    "    \n",
    "    # Add some random key-value pairs from other people\n",
    "    if len(keys) < max_pairs:\n",
    "        other_names = [name for name in DB.keys() if name != entity]\n",
    "        random.shuffle(other_names)\n",
    "        \n",
    "        for other_name in other_names[:max_pairs - len(keys)]:\n",
    "            other_person_data = DB[other_name]\n",
    "            other_relations = list(other_person_data.keys())\n",
    "            if other_relations:\n",
    "                random_rel = random.choice(other_relations)\n",
    "                random_key_text = f\"{other_name} {random_rel}\"\n",
    "                random_value_text = other_person_data[random_rel]\n",
    "                \n",
    "                keys.append(multihot(random_key_text, VOCAB))\n",
    "                values.append(multihot(random_value_text, VOCAB))\n",
    "    \n",
    "    return keys, values, target_idx, person_data[relation] if relation and relation in person_data else None\n",
    "\n",
    "def answer_question(question, model, DB, vocab, Y_matrix):\n",
    "    \"\"\"\n",
    "    Answer a natural language question using the trained KVMemNet model\n",
    "    \n",
    "    Args:\n",
    "        question: Natural language question string\n",
    "        model: Trained KVMemNet model\n",
    "        DB: Database dictionary\n",
    "        vocab: Vocabulary object\n",
    "        Y_matrix: Matrix of all possible values\n",
    "    \n",
    "    Returns:\n",
    "        answer_text: The predicted answer as text\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Extract entity and relation from question\n",
    "    entity, relation = extract_entity_and_relation(question)\n",
    "    print(f\"Extracted entity: '{entity}', relation: '{relation}'\")\n",
    "    \n",
    "    # Get relevant key-value pairs\n",
    "    keys, values, target_idx, correct_answer = get_relevant_keys_values(entity, relation, DB)\n",
    "    \n",
    "    if not keys:\n",
    "        return \"Sorry, I couldn't find information about that person.\"\n",
    "    \n",
    "    # Convert question to bag of words\n",
    "    question_bow = multihot(question, vocab)\n",
    "    question_tensor = torch.tensor(question_bow, dtype=torch.float32).to(device)\n",
    "    \n",
    "    # Convert keys and values to tensors\n",
    "    keys_tensor = torch.stack([torch.tensor(k, dtype=torch.float32) for k in keys]).to(device)\n",
    "    values_tensor = torch.stack([torch.tensor(v, dtype=torch.float32) for v in values]).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Forward pass through model\n",
    "        output = model(question_tensor, keys_tensor, values_tensor)\n",
    "        \n",
    "        # Embed all values in Y using layer B\n",
    "        Y_embedded = model.B(Y_matrix)\n",
    "        \n",
    "        # Compute similarity scores\n",
    "        similarity_scores = torch.matmul(Y_embedded, output)\n",
    "        \n",
    "        # Get the predicted answer\n",
    "        predicted_idx = torch.argmax(similarity_scores).item()\n",
    "        predicted_bow = Y_matrix[predicted_idx].cpu().numpy()\n",
    "        \n",
    "        # Convert back to text (this is a simplified approach)\n",
    "        # In practice, you'd want to store the original text with each bow vector\n",
    "        predicted_tokens = []\n",
    "        for i, val in enumerate(predicted_bow):\n",
    "            if val > 0:\n",
    "                predicted_tokens.append(vocab.index2word(i))\n",
    "        \n",
    "        predicted_text = ' '.join(predicted_tokens)\n",
    "        \n",
    "        # Show debug info\n",
    "        print(f\"Predicted index: {predicted_idx}\")\n",
    "        print(f\"Correct answer: {correct_answer}\")\n",
    "        print(f\"Predicted tokens: {predicted_tokens}\")\n",
    "        \n",
    "        return predicted_text\n",
    "\n",
    "# Test the question answering function\n",
    "print(\"Testing question answering function...\")\n",
    "\n",
    "# Test with the provided question\n",
    "test_question = \"When was alexander hamilton born?\"\n",
    "answer = answer_question(test_question, real_model, DB, VOCAB, Y_matrix)\n",
    "print(f\"Question: {test_question}\")\n",
    "print(f\"Answer: {answer}\")\n",
    "print()\n",
    "\n",
    "# Test with a few more questions\n",
    "test_questions = [\n",
    "    \"Where was Alexander Hamilton born?\",\n",
    "    \"What party was Alexander Hamilton in?\",\n",
    "    \"Who was Alexander Hamilton married to?\",\n",
    "    \"What office did Alexander Hamilton hold?\"\n",
    "]\n",
    "\n",
    "for q in test_questions:\n",
    "    try:\n",
    "        answer = answer_question(q, real_model, DB, VOCAB, Y_matrix)\n",
    "        print(f\"Question: {q}\")\n",
    "        print(f\"Answer: {answer}\")\n",
    "        print()\n",
    "    except Exception as e:\n",
    "        print(f\"Error with question '{q}': {e}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N27-OtrzpT-u"
   },
   "source": [
    "# Part F: Reporting (15 points)\n",
    "\n",
    "Your report should answer the following three questions:\n",
    "\n",
    " **Q1:** What pre-processing of the data did you do? What motivated the design decisions and how did it impact training and any processing of natural language questions (Parts A and D)?\n",
    "\n",
    " Hint: This should help one understand any code modifications you made in Parts A and the first part of Part D. But you shouldn't use this to document your code (hopefully you commented your code with code comments and text cells above), but to justify your choices as well as to explain what worked and what didn't work.\n",
    "\n",
    " **Q2:** Report on your training on the real data (Part D). Show your loss curve and report on the testing accuracy. There are many ways to implement the training loop, particularly with the choice of keys and values. What decisions did you make when developing your training loop? Justify your decisions. How did they impact the training?\n",
    "\n",
    " Hint: This assignment doesn't grade you on how well your model learns---your solution will not be perfect. We focus more on how you worked through the process. This part of the report should show how well your solution worked, but also the intuition for why it works, and to document the things you tried that didn't work.\n",
    "\n",
    " **Q3:** Describe your technique on how you process natural language questions (Part E). Provide some examples of your technique answering questions correctly and some examples of your technique answering questions incorrectly. Discuss what causes the failure cases.\n",
    "\n",
    " Hint: You are not penalized for incorrectly answered questions---your model will not be perfect---we are looking for honest reflection. Preferably, show the example as code blocks running your model with notebook outputs saved.\n",
    "\n",
    " We have provided three prompts below. You can create as many text and code cells as necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V8dovCuyuEvL"
   },
   "source": [
    "**Q1: Report on Data Pre-processing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2qhr3QwvuLBQ"
   },
   "source": [
    "Your text here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3VBW1vSduN4U"
   },
   "source": [
    "**Q2: Report on Training and Testing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PT_9MtT2uQ7I"
   },
   "source": [
    "Your text here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d6YimJ0KuScb"
   },
   "source": [
    "**Q3: Report on Model Use**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j1majVIPuVEP"
   },
   "source": [
    "Your text here."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
